---
- name: Include prerequisites tasks
  include_tasks: "{{ playbook_dir }}/../roles/OSdetection/tasks/prerequisites.yml"

- name: Install pre-requisite packages on Amazon Linux
  include_tasks: "{{ playbook_dir }}/../roles/k8s_setup/templates/prerequisites.yml"

# roles/k8s_setup/tasks/main.yml

- name: Debug vars
  debug:
    var: k8s_packages

# Step 0: Prerequisites
- name: Install Kubernetes pre-requisite packages
  include_tasks: "{{ playbook_dir }}/../roles/k8s_setup/templates/k8s-prerequisites.yml"

- name: Install crictl binary
  get_url:
    url: https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.29.0/crictl-v1.29.0-linux-amd64.tar.gz
    dest: /tmp/crictl.tar.gz

- name: Extract crictl to /usr/local/bin
  unarchive:
    src: /tmp/crictl.tar.gz
    dest: /usr/local/bin/
    remote_src: yes

# Step 1: Prepare binary path
- name: Create /usr/local/bin if not exists
  file:
    path: /usr/local/bin
    state: directory
    mode: '0755'

# Step 2: Install kubeadm, kubelet, kubectl
- name: Install kubeadm, kubelet, kubectl binaries
  get_url:
    url: "https://dl.k8s.io/release/{{ kube_version }}/bin/linux/amd64/{{ item }}"
    dest: "/usr/local/bin/{{ item }}"
    mode: '0755'
  loop:
    - kubelet
    - kubeadm
    - kubectl

# Step 3: kubelet systemd
- name: Create kubelet systemd unit file
  copy:
    dest: /etc/systemd/system/kubelet.service
    mode: '0644'
    content: |
      [Unit]
      Description=kubelet: The Kubernetes Node Agent
      Documentation=https://kubernetes.io/docs/
      After=network-online.target
      Wants=network-online.target

      [Service]
      ExecStart=/usr/local/bin/kubelet
      Restart=always
      StartLimitInterval=0
      RestartSec=10
      KillMode=process

      [Install]
      WantedBy=multi-user.target

- name: Ensure kubelet.service.d directory exists
  file:
    path: /etc/systemd/system/kubelet.service.d
    state: directory
    mode: '0755'

- name: Create kubelet kubeadm drop-in
  copy:
    dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
    content: |
      [Service]
      Environment="KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.conf"
      Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
      ExecStart=
      ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS
    mode: '0644'

# Step 4: Reload systemd
- name: Reload systemd daemon
  command: systemctl daemon-reload

- name: Re-exec systemd process
  command: systemctl daemon-reexec

# Step 5: Start kubelet
- name: Enable and start kubelet
  systemd:
    name: kubelet
    enabled: yes
    state: started

# Step 6: Install Helm
- name: Download Helm install script
  get_url:
    url: https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
    dest: /tmp/get_helm.sh
    mode: '0755'

- name: Install Helm
  shell: /tmp/get_helm.sh
  args:
    creates: /usr/local/bin/helm

# Step 7: Docker install & start
- name: Install Docker
  include_tasks: "{{ playbook_dir }}/../roles/k8s_setup/templates/docker-install.yml"

- name: Enable and start Docker
  systemd:
    name: docker
    enabled: yes
    state: started

# --- Idempotency Checks for Kubernetes Initialization & Self-Healing ---

- name: Check if Kubernetes master configuration exists
  ansible.builtin.stat:
    path: /etc/kubernetes/admin.conf
  register: k8s_config_exists

- name: Debug - Kubernetes config exists check
  debug:
    msg: "Kubernetes admin.conf exists: {{ k8s_config_exists.stat.exists }}"

# Step 8: Swap off (made idempotent)
- name: Ensure swap is disabled (temporarily)
  ansible.builtin.command: swapoff -a
  when: ansible_facts['swaptotal_mb'] > 0 # Only run if swap is active

- name: Ensure swap is disabled in fstab (persistently)
  ansible.builtin.replace:
    path: /etc/fstab
    regexp: '^(\s*)([^#]+\s+)(\w+\s+)(\w+\s+defaults.*swap)\s*$'
    replace: '#\1\2\3\4'
    backup: yes
  when: ansible_facts['swaptotal_mb'] > 0 # Only attempt if swap was active
  notify: Reload systemd daemon # To ensure fstab changes are picked up if not rebooting

# --- Health Check and Conditional Fix for Existing Cluster ---
- name: Check Kubernetes API server health (if admin.conf exists)
  ansible.builtin.shell: |
    KUBECONFIG=/etc/kubernetes/admin.conf \
    kubectl cluster-info > /dev/null 2>&1
  register: k8s_api_health_check
  failed_when: false # Do not fail the task if kubectl fails to connect
  changed_when: false # This is a check, not a change
  when: k8s_config_exists.stat.exists

# Initialize k8s_is_unhealthy_but_exists to false by default
- name: Set fact if Kubernetes is unhealthy but config exists
  set_fact:
    k8s_is_unhealthy_but_exists: "{{ k8s_config_exists.stat.exists and k8s_api_health_check.rc != 0 }}"
  # Removed 'when' condition to ensure it's always defined

- name: Debug - Kubernetes is unhealthy but exists?
  debug:
    msg: "Kubernetes is unhealthy but exists: {{ k8s_is_unhealthy_but_exists }}"
  when: k8s_config_exists.stat.exists is defined # Only debug if k8s_config_exists was run

# Set a fact to explicitly check if admin.conf is absent
- name: Set fact - admin_conf_absent
  set_fact:
    admin_conf_absent: "{{ not k8s_config_exists.stat.exists }}"

- name: Debug - admin_conf_absent
  debug:
    msg: "admin_conf_absent is: {{ admin_conf_absent }}"

# Step 9: kubeadm init (will run if admin.conf doesn't exist)
- name: Initialize Kubernetes cluster
  ansible.builtin.command: kubeadm init --pod-network-cidr=192.168.0.0/16 --ignore-preflight-errors=all
  register: kubeadm_init_result
  changed_when: kubeadm_init_result.rc == 0 and "Your Kubernetes control-plane has initialized successfully!" in kubeadm_init_result.stdout
  failed_when: kubeadm_init_result.rc != 0 and "already initialized" not in kubeadm_init_result.stderr
  # Only run if admin.conf does NOT exist
  when: admin_conf_absent # Using the new explicit fact
  become: yes # Ensure it runs with sudo

# Define a variable to control execution of post-init steps
- name: Set fact if Kubernetes was initialized or already exists
  set_fact:
    k8s_is_ready_or_initialized: "{{ k8s_config_exists.stat.exists or (kubeadm_init_result is defined and 'changed' in kubeadm_init_result and kubeadm_init_result.changed) }}"
  # IMPORTANT: This task is now placed AFTER kubeadm init to ensure kubeadm_init_result is potentially defined.
  # The 'is defined' and 'in kubeadm_init_result' checks handle cases where kubeadm_init_result is not set (e.g., if kubeadm init was skipped).

- name: Debug - k8s_is_ready_or_initialized
  debug:
    msg: "Kubernetes is ready or was initialized in this run: {{ k8s_is_ready_or_initialized }}"


# --- Attempt to fix container runtime issues and force API server restart ---
# This block runs if the cluster is not healthy OR is being initialized for the first time
- name: Pull recommended Kubernetes pause image (v3.9)
  ansible.builtin.shell: |
    # Detect container runtime and pull image accordingly
    if command -v docker &> /dev/null; then
      sudo docker pull registry.k8s.io/pause:3.9
    elif command -v crictl &> /dev/null; then
      sudo crictl pull registry.k8s.io/pause:3.9
    else
      echo "No supported container runtime (docker or containerd) found to pull pause image."
      exit 1
    fi
  register: pull_pause_image_result
  changed_when: "'Pulled' in pull_pause_image_result.stdout or 'Image is up to date' not in pull_pause_image_result.stdout"
  # Simplified 'when' condition to ensure it runs if config exists or if it's a fresh install path
  when: admin_conf_absent or k8s_is_unhealthy_but_exists or (k8s_config_exists.stat.exists and k8s_api_health_check.rc == 0)
  become: yes

- name: Restart container runtime service to apply changes
  ansible.builtin.systemd:
    name: "{{ 'docker' if ansible_facts['service_mgr'] == 'systemd' and ansible_facts['services']['docker.service'] is defined and ansible_facts['services']['docker.service']['state'] == 'running' else 'containerd' }}"
    state: restarted
    daemon_reload: yes
  when:
    - (admin_conf_absent or k8s_is_unhealthy_but_exists or (k8s_config_exists.stat.exists and k8s_api_health_check.rc == 0))
    - ansible_facts['service_mgr'] == 'systemd'
    - (ansible_facts['services']['docker.service'] is defined and ansible_facts['services']['docker.service']['state'] == 'running') or
      (ansible_facts['services']['containerd.service'] is defined and ansible_facts['services']['containerd.service']['state'] == 'running')
  become: yes

- name: Force restart kube-apiserver static pod (if unhealthy but config exists)
  ansible.builtin.file:
    path: /etc/kubernetes/manifests/kube-apiserver.yaml
    state: absent # Remove the manifest to force recreation
  when: k8s_is_unhealthy_but_exists
  become: yes
  notify: Recreate kube-apiserver manifest # Handler to put it back after a short delay


# --- Manual IP Input ---
- name: Prompt for private IP address
  ansible.builtin.pause:
    prompt: "Please enter the private IP address of this machine (e.g., 10.0.0.10)"
    echo: yes
  register: private_ip_input
  when: k8s_is_ready_or_initialized # Only prompt if K8s setup is relevant

- name: Set private_ip from user input
  set_fact:
    private_ip: "{{ private_ip_input.user_input }}"
  when: k8s_is_ready_or_initialized


# Step 10: Skip setting /root/.kube/config, use admin.conf directly

# Step 10.1: Wait until kube-apiserver is ready before modifying
- name: Pause before updating manifest (only if cluster was just initialized or is already running)
  ansible.builtin.pause:
    seconds: 30
  when: k8s_is_ready_or_initialized

# # Step 10.2: Modify kube-apiserver bind address
# - name: Replace --bind-address in kube-apiserver.yaml (only if cluster was just initialized or is already running)
#   ansible.builtin.replace:
#     path: /etc/kubernetes/manifests/kube-apiserver.yaml
#     regexp: '--bind-address=127\.0\.0\.1'
#     replace: '--bind-address={{ private_ip }}'
#   when: k8s_is_ready_or_initialized
#   notify: Restart kubelet # This notify might be redundant if the file task already triggers recreation



# Step: Set KUBECONFIG env var fact (Best Practice - For use in future tasks)
- name: Set KUBECONFIG environment fact
  set_fact:
    kubeconfig_env:
      KUBECONFIG: "/etc/kubernetes/admin.conf"
  when: k8s_is_ready_or_initialized


- name: Echo KUBECONFIG variable
  command: echo $KUBECONFIG
  when: k8s_is_ready_or_initialized


# Step 10.3: Wait until kube-apiserver pod is Running (only if cluster was just initialized or is already running)
- name: Wait for kube-apiserver pod
  ansible.builtin.shell: |
    KUBECONFIG=/etc/kubernetes/admin.conf \
    kubectl get pod -n kube-system -l component=kube-apiserver -o jsonpath='{.items[0].status.phase}'
  register: kube_apiserver_status
  until: kube_apiserver_status.stdout == "Running"
  retries: 20
  delay: 15
  when: k8s_is_ready_or_initialized

# Step 10.4: Health check over private IP (only if cluster was just initialized or is already running)
- name: Wait for Kubernetes API server to become available
  ansible.builtin.uri:
    url: https://{{ private_ip }}:6443/healthz
    method: GET
    validate_certs: no
    status_code: 200
  register: result
  retries: 20
  delay: 15
  when: k8s_is_ready_or_initialized

# Step 10.5: Copy admin.conf to ec2-user's .kube directory and set permissions
- name: Ensure .kube directory exists
  ansible.builtin.file:
    path: "{{ ansible_facts['user_dir'] }}/.kube" # Use ansible_facts for home dir
    state: directory
    owner: "{{ ansible_user_id }}" # Set owner to the connecting user (e.g., ec2-user)
    group: "{{ ansible_user_gid }}" # Set group to the connecting user's group
    mode: '0755'
  become: true # This task needs root to create directory for another user
  become_user: root

- name: Copy admin.conf to machine's .kube/config
  ansible.builtin.copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ ansible_facts['user_dir'] }}/.kube/config" # Destination in user's home
    owner: "{{ ansible_user_id }}"
    group: "{{ ansible_user_gid }}"
    mode: '0604' # Owner read/write only, secure permissions
    remote_src: true # Source file is on the remote machine (EC2 instance itself)
  become: true # This task needs root to read from /etc/kubernetes/admin.conf
  become_user: root

# Step 11: Apply Calico CNI
- name: Install Calico CNI
  ansible.builtin.shell: kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml
  environment:
    KUBECONFIG: "{{ ansible_facts['user_dir'] }}/.kube/config"  # Use the new path!
  args:
    creates: /etc/cni/net.d/calico-kubeconfig
  become: true # kubectl apply often needs root (or relevant permissions)
  become_user: root

# Step 11.1: Remove the control-plane taint
- name: Remove control-plane taint from the node
  ansible.builtin.shell: |
    # Get the node name
    NODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}' --kubeconfig="{{ ansible_facts['user_dir'] }}/.kube/config")
    # Taint the node
    kubectl taint nodes "$NODE_NAME" node-role.kubernetes.io/control-plane- --kubeconfig="{{ ansible_facts['user_dir'] }}/.kube/config"
  register: taint_result
  changed_when: "'NoSchedule' in taint_result.stdout or 'NoSchedule' in taint_result.stderr"
  environment:
    KUBECONFIG: "{{ ansible_facts['user_dir'] }}/.kube/config"  # Ensure kubectl uses the accessible config
  become: true
  become_user: root

- name: Verify taint removal
  ansible.builtin.shell: |
    NODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}' --kubeconfig="{{ ansible_facts['user_dir'] }}/.kube/config")
    # Use grep -q to make it quiet and just check exit code.
    # The regex "Taints:\s*<none>" will match "Taints:" followed by any number of spaces, then "<none>".
    kubectl describe node "$NODE_NAME" | grep -q "Taints:\s*<none>"
  register: taint_check
  # No 'until' needed; the success/failure is determined by grep's exit code
  retries: 5 # These retries are for the shell command itself, in case kubectl isn't ready immediately
  delay: 10
  environment:
    KUBECONFIG: "{{ ansible_facts['user_dir'] }}/.kube/config" 
  become: true
  become_user: root
  changed_when: false # This task is only for verification, not changing state
  failed_when: taint_check.rc != 0 # Fail if grep -q didn't find the string (rc=1)


# Step 12: Grant read permission to /etc/kubernetes/admin.conf
- name:  Grant read permission to /etc/kubernetes/admin.conf
  command: chmod o+r /etc/kubernetes/admin.conf
