---
# Step 8: Swap off (made idempotent)
- name: Ensure swap is disabled (temporarily)
  ansible.builtin.command: swapoff -a
  when: ansible_facts['swaptotal_mb'] > 0 # Only run if swap is active

- name: Ensure swap is disabled in fstab (persistently)
  ansible.builtin.replace:
    path: /etc/fstab
    regexp: '^(\s*)([^#]+\s+)(\w+\s+)(\w+\s+defaults.*swap)\s*$'
    replace: '#\1\2\3\4'
    backup: yes
  when: ansible_facts['swaptotal_mb'] > 0 # Only attempt if swap was active
  notify: Reload systemd daemon # To ensure fstab changes are picked up if not rebooting
- name: Include prerequisites tasks
  include_tasks: "{{ playbook_dir }}/../roles/OSdetection/tasks/prerequisites.yml"






- name: Install pre-requisite packages on Amazon Linux
  include_tasks: "{{ playbook_dir }}/../roles/k8s_setup/templates/prerequisites.yml"

- name: Debug vars
  debug:
    var: k8s_packages

# Step 0: Prerequisites
- name: Install Kubernetes pre-requisite packages
  include_tasks: "{{ playbook_dir }}/../roles/k8s_setup/templates/k8s-prerequisites.yml"

- name: Install crictl binary
  get_url:
    url: https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.29.0/crictl-v1.29.0-linux-amd64.tar.gz
    dest: /tmp/crictl.tar.gz

- name: Extract crictl to /usr/local/bin
  unarchive:
    src: /tmp/crictl.tar.gz
    dest: /usr/local/bin/
    remote_src: yes



- name: Create an empty file for the containerd module
  copy:
        content: ""
        dest: /etc/modules-load.d/containerd.conf
        force: no

- name: Configure modules for containerd
  blockinfile:
        path: /etc/modules-load.d/containerd.conf
        block: |
          overlay
          br_netfilter

- name: Create an empty file for K8S sysctl parameters
  copy:
        content: ""
        dest: /etc/sysctl.d/99-kubernetes-cri.conf
        force: no

- name: Configure sysctl parameters for K8S
  lineinfile:
        path: /etc/sysctl.d/99-kubernetes-cri.conf
        line: "{{ item }}"
  with_items:
        - "net.bridge.bridge-nf-call-iptables  = 1"
        - "net.ipv4.ip_forward                 = 1"
        - "net.bridge.bridge-nf-call-ip6tables = 1"

- name: Apply sysctl parameters
  command: sysctl --system

- name: Install APT Transport HTTPS
  apt:
        name: apt-transport-https
        state: present

- name: Add Docker apt-key
  get_url:
        url: https://download.docker.com/linux/ubuntu/gpg
        dest: /etc/apt/keyrings/docker-apt-keyring.asc
        mode: "0644"
        force: true

- name: Add Docker's APT repo
  apt_repository:
        repo: "deb [arch={{ 'amd64' if ansible_architecture == 'x86_64' else 'arm64' }} signed-by=/etc/apt/keyrings/docker-apt-keyring.asc] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable"
        state: present
        update_cache: yes

- name: Add Kubernetes apt-key
  get_url:
        url: https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key
        dest: /etc/apt/keyrings/kubernetes-apt-keyring.asc
        mode: "0644"
        force: true

- name: Add Kubernetes APT repository
  apt_repository:
        repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.asc] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /"
        state: present
        update_cache: yes

- name: Install containerd
  apt:
        name: containerd.io
        state: present

- name: Create containerd directory
  file:
        path: /etc/containerd
        state: directory

- name: Add containerd configuration
  shell: /usr/bin/containerd config default > /etc/containerd/config.toml

- name: Configuring Systemd cgroup driver for containerd
  lineinfile:
        path: /etc/containerd/config.toml
        regexp: "            SystemdCgroup = false"
        line: "            SystemdCgroup = true"

- name: Enable the containerd service and start service
  systemd:
        name: containerd
        state: restarted
        enabled: yes
        daemon-reload: yes

- name: Load br_netfilter kernel module
  modprobe:
        name: br_netfilter
        state: present

- name: Set bridge-nf-call-iptables
  sysctl:
        name: net.bridge.bridge-nf-call-iptables
        value: 1

- name: Set ip_forward
  sysctl:
        name: net.ipv4.ip_forward
        value: 1






# Step 1: Prepare binary path
- name: Create /usr/local/bin if not exists
  file:
    path: /usr/local/bin
    state: directory
    mode: '0755'

# Step 2: Install kubeadm, kubelet, kubectl
- name: Install kubeadm, kubelet, kubectl binaries
  get_url:
    url: "https://dl.k8s.io/release/{{ kube_version }}/bin/linux/amd64/{{ item }}"
    dest: "/usr/local/bin/{{ item }}"
    mode: '0755'
  loop:
    - kubelet=1.29.*
    - kubeadm=1.29.*
    - kubectl

# Step 3: kubelet systemd
- name: Create kubelet systemd unit file
  copy:
    dest: /etc/systemd/system/kubelet.service
    mode: '0644'
    content: |
      [Unit]
      Description=kubelet: The Kubernetes Node Agent
      Documentation=https://kubernetes.io/docs/
      After=network-online.target
      Wants=network-online.target

      [Service]
      ExecStart=/usr/local/bin/kubelet
      Restart=always
      StartLimitInterval=0
      RestartSec=10
      KillMode=process

      [Install]
      WantedBy=multi-user.target

- name: Ensure kubelet.service.d directory exists
  file:
    path: /etc/systemd/system/kubelet.service.d
    state: directory
    mode: '0755'

- name: Create kubelet kubeadm drop-in
  copy:
    dest: /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
    content: |
      [Service]
      Environment="KUBELET_KUBECONFIG_ARGS=--kubeconfig=/etc/kubernetes/kubelet.conf"
      Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
      ExecStart=
      ExecStart=/usr/local/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS
    mode: '0644'

# Step 4: Reload systemd
- name: Reload systemd daemon
  command: systemctl daemon-reload

- name: Re-exec systemd process
  command: systemctl daemon-reexec

# Step 5: Start kubelet
- name: Enable and start kubelet
  systemd:
    name: kubelet
    enabled: yes
    state: started

# # Step 6: Install Helm
# - name: Download Helm install script
#   get_url:
#     url: https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
#     dest: /tmp/get_helm.sh
#     mode: '0755'

# - name: Install Helm
#   shell: /tmp/get_helm.sh
#   args:
#     creates: /usr/local/bin/helm




# Step 7: Docker install & start
- name: Install Docker
  include_tasks: "{{ playbook_dir }}/../roles/k8s_setup/templates/docker-install.yml"

- name: Enable and start Docker
  systemd:
    name: docker
    enabled: yes
    state: started



- name: Load br_netfilter kernel module
  modprobe:
    name: br_netfilter
    state: present

- name: Set bridge-nf-call-iptables
  sysctl:
    name: net.bridge.bridge-nf-call-iptables
    value: 1

- name: Set ip_forward
  sysctl:
    name: net.ipv4.ip_forward
    value: 1


# --- Idempotency Checks for Kubernetes Initialization & Self-Healing ---

- name: Check if Kubernetes master configuration exists
  ansible.builtin.stat:
    path: /etc/kubernetes/admin.conf
  register: k8s_config_exists

- name: Debug - Kubernetes config exists check
  debug:
    msg: "Kubernetes admin.conf exists: {{ k8s_config_exists.stat.exists }}"



# --- Health Check and Conditional Fix for Existing Cluster ---
- name: Check Kubernetes API server health (if admin.conf exists)
  ansible.builtin.shell: |
    KUBECONFIG=/etc/kubernetes/admin.conf \
    kubectl cluster-info > /dev/null 2>&1
  register: k8s_api_health_check
  failed_when: false # Do not fail the task if kubectl fails to connect
  changed_when: false # This is a check, not a change
  when: k8s_config_exists.stat.exists


# Initialize k8s_is_unhealthy_but_exists to false by default
- name: Set fact if Kubernetes is unhealthy but config exists
  set_fact:
    k8s_is_unhealthy_but_exists: "{{ k8s_config_exists.stat.exists and k8s_api_health_check.rc != 0 }}"


- name: Debug - Kubernetes is unhealthy but exists?
  debug:
    msg: "Kubernetes is unhealthy but exists: {{ k8s_is_unhealthy_but_exists }}"
  when: k8s_config_exists.stat.exists is defined # Only debug if k8s_config_exists was run


# Set a fact to explicitly check if admin.conf is absent
- name: Set fact - admin_conf_absent
  set_fact:
    admin_conf_absent: "{{ not k8s_config_exists.stat.exists }}"

- name: Debug - admin_conf_absent
  debug:
    msg: "admin_conf_absent is: {{ admin_conf_absent }}"


# Step 9: kubeadm init (will run if admin.conf doesn't exist)
- name: Initialize Kubernetes cluster
  ansible.builtin.command: kubeadm init --pod-network-cidr=192.168.0.0/16 --ignore-preflight-errors=all
  register: kubeadm_init_result
  changed_when: kubeadm_init_result.rc == 0 and "Your Kubernetes control-plane has initialized successfully!" in kubeadm_init_result.stdout
  failed_when: kubeadm_init_result.rc != 0 and "already initialized" not in kubeadm_init_result.stderr
  # Only run if admin.conf does NOT exist
  when: admin_conf_absent # Using the new explicit fact
  become: yes # Ensure it runs with sudo


# Define a variable to control execution of post-init steps
- name: Set fact if Kubernetes was initialized or already exists
  set_fact:
    k8s_is_ready_or_initialized: "{{ k8s_config_exists.stat.exists or (kubeadm_init_result is defined and 'changed' in kubeadm_init_result and kubeadm_init_result.changed) }}"
  # IMPORTANT: This task is now placed AFTER kubeadm init to ensure kubeadm_init_result is potentially defined.
  # The 'is defined' and 'in kubeadm_init_result' checks handle cases where kubeadm_init_result is not set (e.g., if kubeadm init was skipped).

- name: Debug - k8s_is_ready_or_initialized
  debug:
    msg: "Kubernetes is ready or was initialized in this run: {{ k8s_is_ready_or_initialized }}"


# --- Attempt to fix container runtime issues and force API server restart ---
# This block runs if the cluster is not healthy OR is being initialized for the first time
- name: Pull recommended Kubernetes pause image (v3.9)
  ansible.builtin.shell: |
    # Detect container runtime and pull image accordingly
    if command -v docker &> /dev/null; then
      sudo docker pull registry.k8s.io/pause:3.9
    elif command -v crictl &> /dev/null; then
      sudo crictl pull registry.k8s.io/pause:3.9
    else
      echo "No supported container runtime (docker or containerd) found to pull pause image."
      exit 1
    fi
  register: pull_pause_image_result
  changed_when: "'Pulled' in pull_pause_image_result.stdout or 'Image is up to date' not in pull_pause_image_result.stdout"
  # Simplified 'when' condition to ensure it runs if config exists or if it's a fresh install path
  when: admin_conf_absent or k8s_is_unhealthy_but_exists or (k8s_config_exists.stat.exists and k8s_api_health_check.rc == 0)
  become: yes

- name: Restart container runtime service to apply changes
  ansible.builtin.systemd:
    name: "{{ 'docker' if ansible_facts['service_mgr'] == 'systemd' and ansible_facts['services']['docker.service'] is defined and ansible_facts['services']['docker.service']['state'] == 'running' else 'containerd' }}"
    state: restarted
    daemon_reload: yes
  when:
    - (admin_conf_absent or k8s_is_unhealthy_but_exists or (k8s_config_exists.stat.exists and k8s_api_health_check.rc == 0))
    - ansible_facts['service_mgr'] == 'systemd'
    - (ansible_facts['services']['docker.service'] is defined and ansible_facts['services']['docker.service']['state'] == 'running') or
      (ansible_facts['services']['containerd.service'] is defined and ansible_facts['services']['containerd.service']['state'] == 'running')
  become: yes

# - name: Force restart kube-apiserver static pod (if unhealthy but config exists)
#   ansible.builtin.file:
#     path: /etc/kubernetes/manifests/kube-apiserver.yaml
#     state: absent # Remove the manifest to force recreation
#   when: k8s_is_unhealthy_but_exists
#   become: yes
#   notify: Recreate kube-apiserver manifest # Handler to put it back after a short delay


# --- Manual IP Input ---
- name: Prompt for private IP address
  ansible.builtin.pause:
    prompt: "Please enter the private IP address of this machine (e.g., 10.0.0.10)"
    echo: yes
  register: private_ip_input
  when: k8s_is_ready_or_initialized # Only prompt if K8s setup is relevant

- name: Set private_ip from user input
  set_fact:
    private_ip: "{{ private_ip_input.user_input }}"
  when: k8s_is_ready_or_initialized


# Step 10: Skip setting /root/.kube/config, use admin.conf directly

# Step 10.1: Wait until kube-apiserver is ready before modifying
- name: Pause before updating manifest (only if cluster was just initialized or is already running)
  ansible.builtin.pause:
    seconds: 150
  when: k8s_is_ready_or_initialized

# Step 10.2: Modify kube-apiserver bind address
- name: Replace --bind-address in kube-apiserver.yaml (only if cluster was just initialized or is already running)
  ansible.builtin.replace:
    path: /etc/kubernetes/manifests/kube-apiserver.yaml
    regexp: '--bind-address=127\.0\.0\.1'
    replace: '--bind-address={{ private_ip }}'
  when: k8s_is_ready_or_initialized
  notify: Restart kubelet # This notify might be redundant if the file task already triggers recreation



# Step: Set KUBECONFIG env var fact (Best Practice - For use in future tasks)
- name: Set KUBECONFIG environment fact
  set_fact:
    kubeconfig_env:
      KUBECONFIG: "/etc/kubernetes/admin.conf"
  when: k8s_is_ready_or_initialized


- name: Echo KUBECONFIG variable
  ansible.builtin.shell: echo "KUBECONFIG is set to {{ kubeconfig_env.KUBECONFIG }}"
  when: k8s_is_ready_or_initialized


# Step 10.3: Wait until kube-apiserver pod is Running (only if cluster was just initialized or is already running)
- name: Wait for kube-apiserver pod
  ansible.builtin.shell: |
    KUBECONFIG=/etc/kubernetes/admin.conf \
    kubectl get pod -n kube-system -l component=kube-apiserver -o jsonpath='{.items[0].status.phase}'
    
  register: kube_apiserver_status
  until: kube_apiserver_status.stdout == "Running"
  retries: 20
  delay: 15
  environment: "{{ kubeconfig_env }}"
  become: true
  become_user: root
  when: k8s_is_ready_or_initialized

# Step 10.4: Health check over private IP (only if cluster was just initialized or is already running)
- name: Wait for Kubernetes API server to become available
  ansible.builtin.uri:
    url: https://{{ private_ip }}:6443/healthz
    method: GET
    validate_certs: no
    status_code: 200
  register: result
  retries: 20
  delay: 15
  when: k8s_is_ready_or_initialized

# Step 10.5: Copy admin.conf to ec2-user's .kube directory and set permissions
- name: Ensure .kube directory exists
  ansible.builtin.file:
    path: "{{ ansible_facts['user_dir'] }}/.kube" # Use ansible_facts for home dir
    state: directory
    owner: "{{ ansible_user_id }}" # Set owner to the connecting user (e.g., ec2-user)
    group: "{{ ansible_user_gid }}" # Set group to the connecting user's group
    mode: '0755'
  become: true # This task needs root to create directory for another user
  become_user: root

- name: Copy admin.conf to machine's .kube/config
  ansible.builtin.copy:
    src: /etc/kubernetes/admin.conf
    dest: "{{ ansible_facts['user_dir'] }}/.kube/config" # Destination in user's home
    owner: "{{ ansible_user_id }}"
    group: "{{ ansible_user_gid }}"
    mode: '0604' # Owner read/write only, secure permissions
    remote_src: true # Source file is on the remote machine (EC2 instance itself)
  become: true # This task needs root to read from /etc/kubernetes/admin.conf
  become_user: root

  
# ✅ Step 11.1: Remove control-plane taint (if exists)
- name: Check if control-plane taint exists
  ansible.builtin.shell: |
    kubectl describe node $(kubectl get nodes -o jsonpath='{.items[0].metadata.name}') | grep -q "node-role.kubernetes.io/control-plane:NoSchedule"
  register: control_plane_taint_check
  ignore_errors: true
  changed_when: false
  environment: "{{ kubeconfig_env }}"
  become: true
  become_user: root

- name: Remove control-plane taint
  shell: |
    NODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
    kubectl taint nodes "$NODE_NAME" node-role.kubernetes.io/control-plane- || true
  when: control_plane_taint_check.rc == 0
  environment: "{{ kubeconfig_env }}"
  become: true
  become_user: root

- name: Verify control-plane taint removal
  shell: |
    NODE_NAME=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
    kubectl describe node "$NODE_NAME" | grep -q "Taints:\s*<none>"
  register: taint_check
  retries: 5
  delay: 10
  environment: "{{ kubeconfig_env }}"
  failed_when: taint_check.rc != 0
  changed_when: false
  become: true
  become_user: root

# ✅ Step 11.2: Ensure kube-root-ca.crt ConfigMap exists
- name: Ensure kube-root-ca.crt ConfigMap exists in kube-system
  shell: |
    kubectl get configmap kube-root-ca.crt -n kube-system || \
    kubectl create configmap kube-root-ca.crt --from-file=/etc/kubernetes/pki/ca.crt -n kube-system
  register: create_root_ca_cm
  changed_when: "'created' in create_root_ca_cm.stdout"
  environment: "{{ kubeconfig_env }}"
  become: true
  become_user: root

# ✅ Step 11.3: Restart kube-proxy pods [First one ensures the pod doesn’t run on an incomplete/old config during apply.]
- name: Delete kube-proxy pod to restart
  shell: |
    kubectl delete pod -n kube-system -l k8s-app=kube-proxy --force --grace-period=0
  ignore_errors: yes
  environment: "{{ kubeconfig_env }}"
  become: true
  become_user: root

# - name: Patch kube-proxy ConfigMap with clusterCIDR
#   shell: |
#     kubectl -n kube-system patch configmap kube-proxy \
#       --type merge \
#       -p '{"data":{"config.conf":"{\n  \"mode\": \"iptables\",\n  \"clusterCIDR\": \"192.168.0.0/16\"\n}"}}'
#   register: patch_kube_proxy_configmap
#   changed_when: "'configured' in patch_kube_proxy_configmap.stdout or 'patched' in patch_kube_proxy_configmap.stdout or patch_kube_proxy_configmap.rc == 0"
#   environment: "{{ kubeconfig_env }}"
#   become: true
#   become_user: root

- name: Write full kube-proxy ConfigMap YAML
  copy:
    dest: /tmp/kube-proxy-cm.yaml
    content: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: kube-proxy
        namespace: kube-system
      data:
        config.conf: |
          apiVersion: kubeproxy.config.k8s.io/v1alpha1
          kind: KubeProxyConfiguration
          mode: "iptables"
          clusterCIDR: "192.168.0.0/16,fd00::/64"

- name: Apply kube-proxy ConfigMap
  command: kubectl apply -f /tmp/kube-proxy-cm.yaml
  register: apply_kube_proxy_configmap
  changed_when: "'configured' in apply_kube_proxy_configmap.stdout or 'created' in apply_kube_proxy_configmap.stdout or 'unchanged' not in apply_kube_proxy_configmap.stdout"
  environment: "{{ kubeconfig_env }}"
  become: true
  become_user: root


    #Second ensures the pod reloads the new ConfigMap.
- name: Delete kube-proxy pod to restart
  shell: |
    kubectl delete pod -n kube-system -l k8s-app=kube-proxy --force --grace-period=0
  ignore_errors: yes
  environment: "{{ kubeconfig_env }}"
  become: true
  become_user: root

- name: Wait for kube-proxy pod to be Running
  shell: |
    kubectl get pods -n kube-system -l k8s-app=kube-proxy -o jsonpath='{.items[0].status.phase}'
  register: kube_proxy_status
  until: kube_proxy_status.stdout == "Running"
  retries: 10
  delay: 10
  environment: "{{ kubeconfig_env }}"
  become: true
  become_user: root

# Step 12:
- name: Grant read permission to /etc/kubernetes/admin.conf
  command: chmod o+r /etc/kubernetes/admin.conf

- name: Wait for Kubernetes kube-proxy pod to be ready
  ansible.builtin.pause:
    seconds: 130

# ✅ Step 11: Apply Calico CNI
- name: Apply Calico
  command: kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml
  args:
    creates: /etc/cni/net.d/calico-kubeconfig

- name: Wait for calico-node pods to be Running
  shell: |
    pods=$(kubectl get pods -n kube-system -l k8s-app=calico-node -o jsonpath='{.items[*].status.phase}')
    if [ -z "$pods" ]; then
      echo "NotReady"
    elif echo "$pods" | grep -qv "Running"; then
      echo "NotReady"
    else
      echo "Running"
    fi
  register: calico_status
  until: calico_status.stdout == "Running"
  retries: 15
  delay: 15

- name: Restart kubelet to fix CNI plugin issues
  systemd:
    name: kubelet
    state: restarted

- name: Reapply Calico if needed
  command: kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/calico.yaml
  when: calico_status.stdout != "Running"


- name: Waiting for Calico CNI to be ready
  ansible.builtin.pause:
    seconds: 90